{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JahnviAghera/DAA/blob/main/ECG_Processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 983
        },
        "id": "13da9b1b",
        "outputId": "30119dc2-bbdd-4704-e8f0-aaf555afe4c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wfdb\n",
            "  Downloading wfdb-4.3.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: aiohttp>=3.10.11 in /usr/local/lib/python3.12/dist-packages (from wfdb) (3.13.3)\n",
            "Requirement already satisfied: fsspec>=2023.10.0 in /usr/local/lib/python3.12/dist-packages (from wfdb) (2025.3.0)\n",
            "Requirement already satisfied: matplotlib>=3.2.2 in /usr/local/lib/python3.12/dist-packages (from wfdb) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.26.4 in /usr/local/lib/python3.12/dist-packages (from wfdb) (2.0.2)\n",
            "Collecting pandas>=2.2.3 (from wfdb)\n",
            "  Downloading pandas-2.3.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (91 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.8.1 in /usr/local/lib/python3.12/dist-packages (from wfdb) (2.32.4)\n",
            "Requirement already satisfied: scipy>=1.13.0 in /usr/local/lib/python3.12/dist-packages (from wfdb) (1.16.3)\n",
            "Requirement already satisfied: soundfile>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from wfdb) (0.13.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb) (1.22.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.2.2->wfdb) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.2.2->wfdb) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.2.2->wfdb) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.2.2->wfdb) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.2.2->wfdb) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.2.2->wfdb) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.2.2->wfdb) (3.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.2.2->wfdb) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2.3->wfdb) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2.3->wfdb) (2025.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.8.1->wfdb) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.8.1->wfdb) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.8.1->wfdb) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.8.1->wfdb) (2026.1.4)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.12/dist-packages (from soundfile>=0.10.0->wfdb) (2.0.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.12/dist-packages (from aiosignal>=1.4.0->aiohttp>=3.10.11->wfdb) (4.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0->soundfile>=0.10.0->wfdb) (2.23)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.2.2->wfdb) (1.17.0)\n",
            "Downloading wfdb-4.3.0-py3-none-any.whl (163 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.8/163.8 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pandas-2.3.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (12.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m89.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pandas, wfdb\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pandas-2.3.3 wfdb-4.3.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pandas"
                ]
              },
              "id": "9e0bf98ddace460ab0792486e4fa9f85"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import os\n",
        "\n",
        "!pip install wfdb # Install missing library\n",
        "import wfdb\n",
        "\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Dropout, LSTM, Dense, TimeDistributed, Flatten\n",
        "from tensorflow.keras import optimizers, callbacks\n",
        "\n",
        "DATA_PATH = '/content/drive/MyDrive/mit-bih-arrhythmia-database'\n",
        "MODEL_SAVE_PATH = \"saved_models/CNN_LSTM_Oversampled_Aggressive.h5\"\n",
        "\n",
        "SEQUENCE_LENGTH = 3\n",
        "BEAT_LENGTH = 180\n",
        "\n",
        "# Class definition based on AAMI recommendation and previous mapping\n",
        "CLASSES = ['N', 'LBBB', 'RBBB', 'APC', 'AESC', 'ABERR', 'NPC', 'NESC']\n",
        "CLASS_MAP = {\n",
        "    'N': 0, 'L': 1, 'R': 2, 'A': 3,\n",
        "    'e': 4, 'a': 5, 'J': 6, 'j': 7\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "06c01c95"
      },
      "outputs": [],
      "source": [
        "def build_cnn_lstm_model(sequence_length, beat_length, num_classes):\n",
        "    input_shape = (sequence_length, beat_length, 1)\n",
        "\n",
        "    # Input layer\n",
        "    inputs = Input(shape=input_shape)\n",
        "\n",
        "    # TimeDistributed CNN part (Applies CNN to each beat in the sequence)\n",
        "    td_cnn = TimeDistributed(Conv1D(32, 5, activation='relu', padding='same'))(inputs)\n",
        "    td_cnn = TimeDistributed(Conv1D(32, 5, activation='relu', padding='same'))(td_cnn)\n",
        "    td_cnn = TimeDistributed(MaxPooling1D(2))(td_cnn)\n",
        "    td_cnn = TimeDistributed(Dropout(0.25))(td_cnn)\n",
        "\n",
        "    td_cnn = TimeDistributed(Conv1D(64, 5, activation='relu', padding='same'))(td_cnn)\n",
        "    td_cnn = TimeDistributed(Conv1D(64, 5, activation='relu', padding='same'))(td_cnn)\n",
        "    td_cnn = TimeDistributed(MaxPooling1D(2))(td_cnn)\n",
        "    td_cnn = TimeDistributed(Dropout(0.25))(td_cnn)\n",
        "\n",
        "    # Flatten the output of the CNN for the LSTM input\n",
        "    td_cnn_flatten = TimeDistributed(Flatten())(td_cnn)\n",
        "\n",
        "    # LSTM part (Processes the sequence of extracted features)\n",
        "    lstm_out = LSTM(128, activation='tanh', return_sequences=False)(td_cnn_flatten)\n",
        "    lstm_out = Dropout(0.5)(lstm_out)\n",
        "\n",
        "    # Output layer\n",
        "    outputs = Dense(num_classes, activation='softmax')(lstm_out)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "cca29213"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    # 1. Load and Process Data\n",
        "    processor = ECGDataProcessor(SEQUENCE_LENGTH, BEAT_LENGTH)\n",
        "    try:\n",
        "        X, y = processor.load_real_data(DATA_PATH)\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading data: {e}\")\n",
        "        return\n",
        "\n",
        "    # 2. Train / Test Split\n",
        "    X_train_raw, X_test_raw, y_train, y_test = train_test_split(\n",
        "        X, y,\n",
        "        test_size=0.2,\n",
        "        random_state=42,\n",
        "        stratify=y\n",
        "    )\n",
        "\n",
        "    print(f\"Initial Train Shape: {X_train_raw.shape}\")\n",
        "    print(f\"Test Shape: {X_test_raw.shape}\")\n",
        "    print(f\"Initial Class Distribution (Train): {Counter(y_train)}\")\n",
        "\n",
        "    # 3. Aggressive Oversampling\n",
        "    MIN_SAMPLES_TARGET = 500\n",
        "    X_resampled, y_resampled = [], []\n",
        "\n",
        "    for cls in np.unique(y_train):\n",
        "        indices = np.where(y_train == cls)[0]\n",
        "        X_cls = X_train_raw[indices]\n",
        "        y_cls = y_train[indices]\n",
        "        count = len(y_cls)\n",
        "\n",
        "        if count < MIN_SAMPLES_TARGET:\n",
        "            oversample_factor = math.ceil(MIN_SAMPLES_TARGET / count)\n",
        "\n",
        "            X_aug = np.tile(X_cls, (oversample_factor, 1, 1))[:MIN_SAMPLES_TARGET]\n",
        "            y_aug = np.tile(y_cls, oversample_factor)[:MIN_SAMPLES_TARGET]\n",
        "\n",
        "            X_resampled.append(X_aug)\n",
        "            y_resampled.append(y_aug)\n",
        "\n",
        "            print(f\"Oversampling Class {cls} ({CLASSES[cls]}): {count} → {len(y_aug)}\")\n",
        "        else:\n",
        "            X_resampled.append(X_cls)\n",
        "            y_resampled.append(y_cls)\n",
        "\n",
        "    X_train_resampled = np.concatenate(X_resampled, axis=0)\n",
        "    y_train_resampled = np.concatenate(y_resampled, axis=0)\n",
        "\n",
        "    shuffle_idx = np.random.permutation(len(X_train_resampled))\n",
        "    X_train_final = X_train_resampled[shuffle_idx]\n",
        "    y_train_final = y_train_resampled[shuffle_idx]\n",
        "\n",
        "    # 4. Reshape for CNN input\n",
        "    X_train = X_train_final.reshape(-1, SEQUENCE_LENGTH, BEAT_LENGTH, 1)\n",
        "    X_test = X_test_raw.reshape(-1, SEQUENCE_LENGTH, BEAT_LENGTH, 1)\n",
        "\n",
        "    print(f\"\\nFinal Train Shape: {X_train.shape}\")\n",
        "    print(f\"Final Class Distribution: {Counter(y_train_final)}\")\n",
        "\n",
        "    # 5. Log-Smoothed Class Weights\n",
        "    unique, counts = np.unique(y_train_final, return_counts=True)\n",
        "    max_count = counts.max()\n",
        "\n",
        "    class_weights_dict = {}\n",
        "    print(\"\\nFinal Smoothed Class Weights:\")\n",
        "    for cls, count in zip(unique, counts):\n",
        "        weight = np.log(max_count / (count + 1e-6)) + 1.0\n",
        "        class_weights_dict[cls] = min(weight, 7.0)\n",
        "        print(f\"Class {cls} ({CLASSES[cls]}): Count={count}, Weight={class_weights_dict[cls]:.4f}\")\n",
        "\n",
        "    # 6. Build & Compile Model\n",
        "    model = build_cnn_lstm_model(\n",
        "        SEQUENCE_LENGTH,\n",
        "        BEAT_LENGTH,\n",
        "        len(CLASSES)\n",
        "    )\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=optimizers.Adam(learning_rate=1e-4),\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    # 7. Callbacks (Best Model Saving)\n",
        "    os.makedirs(os.path.dirname(MODEL_SAVE_PATH), exist_ok=True)\n",
        "\n",
        "    early_stop = callbacks.EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=15,\n",
        "        restore_best_weights=True\n",
        "    )\n",
        "\n",
        "    reduce_lr = callbacks.ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.5,\n",
        "        patience=5,\n",
        "        min_lr=1e-7,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    best_model_ckpt = callbacks.ModelCheckpoint(\n",
        "        filepath=MODEL_SAVE_PATH,\n",
        "        monitor='val_loss',\n",
        "        save_best_only=True,\n",
        "        save_weights_only=False,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # 8. Train\n",
        "    print(\"\\nStarting training with aggressive oversampling and class weighting...\")\n",
        "    history = model.fit(\n",
        "        X_train,\n",
        "        y_train_final,\n",
        "        epochs=50,\n",
        "        batch_size=32,\n",
        "        validation_split=0.1,\n",
        "        class_weight=class_weights_dict,\n",
        "        callbacks=[early_stop, reduce_lr, best_model_ckpt],\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    print(f\"\\nBest model saved to: {MODEL_SAVE_PATH}\")\n",
        "\n",
        "    # 9. Load Best Model for Evaluation\n",
        "    model = tf.keras.models.load_model(MODEL_SAVE_PATH)\n",
        "    print(\"Loaded best validation model for evaluation.\")\n",
        "\n",
        "    # 10. Evaluate on Test Set\n",
        "    y_pred_probs = model.predict(X_test)\n",
        "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "\n",
        "    print(\"\\nClassification Report (Test Data):\")\n",
        "    print(\n",
        "        classification_report(\n",
        "            y_test,\n",
        "            y_pred,\n",
        "            target_names=CLASSES,\n",
        "            zero_division=0,\n",
        "            digits=4\n",
        "        )\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "15ca11a8"
      },
      "outputs": [],
      "source": [
        "class ECGDataProcessor:\n",
        "    def __init__(self, sequence_length=3, beat_length=180):\n",
        "        self.seq_len = sequence_length\n",
        "        self.beat_len = beat_length\n",
        "        self.scaler = StandardScaler()\n",
        "\n",
        "    def load_real_data(self, record_path):\n",
        "        if not os.path.exists(record_path):\n",
        "            raise FileNotFoundError(f\"The directory '{record_path}' does not exist.\")\n",
        "\n",
        "        records = [f.split('.')[0] for f in os.listdir(record_path) if f.endswith('.dat')]\n",
        "        records = sorted(list(set(records)))\n",
        "\n",
        "        X_all, y_all = [], []\n",
        "\n",
        "        # NOTE: Only printing count, loading happens below\n",
        "        print(f\"Found {len(records)} records. Loading...\")\n",
        "\n",
        "        for record_name in records:\n",
        "            try:\n",
        "                record = wfdb.rdrecord(os.path.join(record_path, record_name))\n",
        "                annotation = wfdb.rdann(os.path.join(record_path, record_name), 'atr')\n",
        "\n",
        "                signal = record.p_signal[:, 0]\n",
        "                signal = self.denoise_signal(signal)\n",
        "\n",
        "                peaks = annotation.sample\n",
        "                labels = annotation.symbol\n",
        "\n",
        "                X_rec, y_rec = self.segment_beats(signal, peaks, labels)\n",
        "                X_all.extend(X_rec)\n",
        "                y_all.extend(y_rec)\n",
        "            except Exception as e:\n",
        "                # print(f\"Skipping {record_name}: {e}\")\n",
        "                continue\n",
        "\n",
        "        if not X_all:\n",
        "            raise ValueError(\"No valid beats were extracted.\")\n",
        "\n",
        "        X_all = np.array(X_all)\n",
        "        y_all = np.array(y_all)\n",
        "\n",
        "        # Scale the data before splitting/reshaping\n",
        "        N, S, B = X_all.shape\n",
        "        X_reshaped = X_all.reshape(-1, B)\n",
        "        # Fit on all data, transform on all data\n",
        "        X_scaled = self.scaler.fit_transform(X_reshaped)\n",
        "        X_final = X_scaled.reshape(N, S, B)\n",
        "\n",
        "        return X_final, y_all\n",
        "\n",
        "    def denoise_signal(self, signal):\n",
        "        # Simple moving average filter\n",
        "        return np.convolve(signal, np.ones(5)/5, mode='same')\n",
        "\n",
        "    def segment_beats(self, signal, peaks, labels):\n",
        "        beats = []\n",
        "        beat_labels = []\n",
        "        half_len = self.beat_len // 2\n",
        "\n",
        "        for i, peak in enumerate(peaks):\n",
        "            if peak < half_len or peak > len(signal) - half_len:\n",
        "                continue\n",
        "\n",
        "            symbol = labels[i]\n",
        "            if symbol in CLASS_MAP:\n",
        "                beat_segment = signal[peak - half_len : peak + half_len]\n",
        "                beats.append(beat_segment)\n",
        "                beat_labels.append(CLASS_MAP[symbol])\n",
        "\n",
        "        # Create sequential input (sequence of beats)\n",
        "        X_seq, y_target = [], []\n",
        "        for i in range(len(beats) - self.seq_len):\n",
        "            sequence = np.array(beats[i : i + self.seq_len])\n",
        "            target = beat_labels[i + self.seq_len] # Label is the beat immediately following the sequence\n",
        "            X_seq.append(sequence)\n",
        "            y_target.append(target)\n",
        "\n",
        "        return X_seq, y_target\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8753b62a",
        "outputId": "f331034e-07e3-40b2-92a9-687463fdc44c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 48 records. Loading...\n",
            "Initial Train Shape: (74596, 3, 180)\n",
            "Test Shape: (18650, 3, 180)\n",
            "Initial Class Distribution (Train): Counter({np.int64(0): 59935, np.int64(1): 6451, np.int64(2): 5793, np.int64(3): 2035, np.int64(7): 183, np.int64(5): 120, np.int64(6): 66, np.int64(4): 13})\n",
            "Oversampling Class 4 (AESC): 13 → 500\n",
            "Oversampling Class 5 (ABERR): 120 → 500\n",
            "Oversampling Class 6 (NPC): 66 → 500\n",
            "Oversampling Class 7 (NESC): 183 → 500\n",
            "\n",
            "Final Train Shape: (76214, 3, 180, 1)\n",
            "Final Class Distribution: Counter({np.int64(0): 59935, np.int64(1): 6451, np.int64(2): 5793, np.int64(3): 2035, np.int64(6): 500, np.int64(4): 500, np.int64(5): 500, np.int64(7): 500})\n",
            "\n",
            "Final Smoothed Class Weights:\n",
            "Class 0 (N): Count=59935, Weight=1.0000\n",
            "Class 1 (LBBB): Count=6451, Weight=3.2290\n",
            "Class 2 (RBBB): Count=5793, Weight=3.3366\n",
            "Class 3 (APC): Count=2035, Weight=4.3828\n",
            "Class 4 (AESC): Count=500, Weight=5.7864\n",
            "Class 5 (ABERR): Count=500, Weight=5.7864\n",
            "Class 6 (NPC): Count=500, Weight=5.7864\n",
            "Class 7 (NESC): Count=500, Weight=5.7864\n",
            "\n",
            "Starting training with aggressive oversampling and class weighting...\n",
            "Epoch 1/50\n",
            "\u001b[1m2144/2144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 230ms/step - accuracy: 0.8633 - loss: 1.2746\n",
            "Epoch 1: val_loss improved from inf to 0.16072, saving model to saved_models/CNN_LSTM_Oversampled_Aggressive.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m2144/2144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m511s\u001b[0m 236ms/step - accuracy: 0.8633 - loss: 1.2744 - val_accuracy: 0.9539 - val_loss: 0.1607 - learning_rate: 1.0000e-04\n",
            "Epoch 2/50\n",
            "\u001b[1m2144/2144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 229ms/step - accuracy: 0.9399 - loss: 0.5271\n",
            "Epoch 2: val_loss improved from 0.16072 to 0.15866, saving model to saved_models/CNN_LSTM_Oversampled_Aggressive.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m2144/2144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m502s\u001b[0m 234ms/step - accuracy: 0.9399 - loss: 0.5270 - val_accuracy: 0.9511 - val_loss: 0.1587 - learning_rate: 1.0000e-04\n",
            "Epoch 3/50\n",
            "\u001b[1m2144/2144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 227ms/step - accuracy: 0.9474 - loss: 0.4347\n",
            "Epoch 3: val_loss improved from 0.15866 to 0.11715, saving model to saved_models/CNN_LSTM_Oversampled_Aggressive.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m2144/2144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m496s\u001b[0m 232ms/step - accuracy: 0.9474 - loss: 0.4347 - val_accuracy: 0.9677 - val_loss: 0.1172 - learning_rate: 1.0000e-04\n",
            "Epoch 4/50\n",
            "\u001b[1m2144/2144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 227ms/step - accuracy: 0.9554 - loss: 0.3735\n",
            "Epoch 4: val_loss did not improve from 0.11715\n",
            "\u001b[1m2144/2144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m497s\u001b[0m 232ms/step - accuracy: 0.9554 - loss: 0.3735 - val_accuracy: 0.9642 - val_loss: 0.1175 - learning_rate: 1.0000e-04\n",
            "Epoch 5/50\n",
            "\u001b[1m2144/2144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 226ms/step - accuracy: 0.9615 - loss: 0.3338\n",
            "Epoch 5: val_loss did not improve from 0.11715\n",
            "\u001b[1m2144/2144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m499s\u001b[0m 230ms/step - accuracy: 0.9615 - loss: 0.3338 - val_accuracy: 0.9656 - val_loss: 0.1194 - learning_rate: 1.0000e-04\n",
            "Epoch 6/50\n",
            "\u001b[1m2144/2144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 226ms/step - accuracy: 0.9618 - loss: 0.3199\n",
            "Epoch 6: val_loss improved from 0.11715 to 0.10928, saving model to saved_models/CNN_LSTM_Oversampled_Aggressive.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m2144/2144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m502s\u001b[0m 231ms/step - accuracy: 0.9618 - loss: 0.3199 - val_accuracy: 0.9676 - val_loss: 0.1093 - learning_rate: 1.0000e-04\n",
            "Epoch 7/50\n",
            "\u001b[1m2144/2144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 227ms/step - accuracy: 0.9680 - loss: 0.2835\n",
            "Epoch 7: val_loss did not improve from 0.10928\n",
            "\u001b[1m2144/2144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m504s\u001b[0m 231ms/step - accuracy: 0.9680 - loss: 0.2835 - val_accuracy: 0.9686 - val_loss: 0.1119 - learning_rate: 1.0000e-04\n",
            "Epoch 8/50\n",
            "\u001b[1m2144/2144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 227ms/step - accuracy: 0.9689 - loss: 0.2727\n",
            "Epoch 8: val_loss improved from 0.10928 to 0.09745, saving model to saved_models/CNN_LSTM_Oversampled_Aggressive.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m2144/2144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m497s\u001b[0m 232ms/step - accuracy: 0.9689 - loss: 0.2727 - val_accuracy: 0.9727 - val_loss: 0.0974 - learning_rate: 1.0000e-04\n",
            "Epoch 9/50\n",
            "\u001b[1m2144/2144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 228ms/step - accuracy: 0.9711 - loss: 0.2580\n",
            "Epoch 9: val_loss improved from 0.09745 to 0.09741, saving model to saved_models/CNN_LSTM_Oversampled_Aggressive.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m2144/2144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m496s\u001b[0m 231ms/step - accuracy: 0.9711 - loss: 0.2580 - val_accuracy: 0.9726 - val_loss: 0.0974 - learning_rate: 1.0000e-04\n",
            "Epoch 10/50\n",
            "\u001b[1m2144/2144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 228ms/step - accuracy: 0.9720 - loss: 0.2447\n",
            "Epoch 10: val_loss did not improve from 0.09741\n",
            "\u001b[1m2144/2144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m499s\u001b[0m 233ms/step - accuracy: 0.9720 - loss: 0.2447 - val_accuracy: 0.9714 - val_loss: 0.1006 - learning_rate: 1.0000e-04\n",
            "Epoch 11/50\n",
            "\u001b[1m2144/2144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 228ms/step - accuracy: 0.9719 - loss: 0.2425\n",
            "Epoch 11: val_loss did not improve from 0.09741\n",
            "\u001b[1m2144/2144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m500s\u001b[0m 232ms/step - accuracy: 0.9719 - loss: 0.2425 - val_accuracy: 0.9710 - val_loss: 0.0976 - learning_rate: 1.0000e-04\n",
            "Epoch 12/50\n",
            "\u001b[1m1155/2144\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m4:02\u001b[0m 246ms/step - accuracy: 0.9776 - loss: 0.2012"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Ensure TensorFlow is not printing excessive warnings\n",
        "    tf.get_logger().setLevel('ERROR')\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1v5ZZzo-VaIvvZEx5SS5oos5ZoWagWXLn",
      "authorship_tag": "ABX9TyNoB1ztGn+L/Zi/J3bficwo",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}